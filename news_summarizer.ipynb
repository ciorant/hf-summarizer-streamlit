{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Install gdown if needed\n",
        "# !pip install gdown\n",
        "\n",
        "# import gdown\n",
        "# gdown.download_folder('your_folder_name')\n"
      ],
      "metadata": {
        "id": "bEiLFgRaNKUc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gFox_bRD6F0z"
      },
      "outputs": [],
      "source": [
        "# Install dependencies if needed\n",
        "# !pip install transformers datasets evaluate rouge-score\n",
        "# !pip install sumy\n",
        "# !pip install streamlit gradio\n",
        "# !pip install nltk\n",
        "# !pip install --upgrade transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "dataset = load_dataset(\"cnn_dailymail\", name=\"3.0.0\")\n",
        "print(dataset)\n",
        "print(dataset[\"train\"][0])"
      ],
      "metadata": {
        "id": "qLvC7vCC6Of0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "RANDOM_SEED=63\n",
        "# Select smaller sets so training is achievable\n",
        "small_train = dataset[\"train\"].shuffle(seed=RANDOM_SEED).select(range(25000))\n",
        "small_val = dataset[\"validation\"].shuffle(seed=RANDOM_SEED).select(range(2000))"
      ],
      "metadata": {
        "id": "qmNhdPZz6pzf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "summarizer = pipeline(\"summarization\",model=\"facebook/bart-large-cnn\")\n",
        "\n",
        "text=dataset[\"test\"][0][\"article\"]\n",
        "print(\"Original\", text[:500],\"...\")\n",
        "print(\"Reference: \", dataset[\"test\"][0][\"highlights\"])\n",
        "\n",
        "summary = summarizer(text, max_length=130, min_length=30, do_sample=False)\n",
        "print(\"Generated:\", summary[0]['summary_text']) #Sanity check\n"
      ],
      "metadata": {
        "id": "c7pJ2srX7Rgg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# If needed:\n",
        "# !pip install sumy\n",
        "import nltk\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "from sumy.parsers.plaintext import PlaintextParser\n",
        "from sumy.nlp.tokenizers import Tokenizer\n",
        "from sumy.summarizers.text_rank import TextRankSummarizer\n",
        "\n",
        "text = dataset[\"test\"][0][\"article\"]\n",
        "\n",
        "parser = PlaintextParser.from_string(text, Tokenizer(\"english\"))\n",
        "summarizer = TextRankSummarizer()"
      ],
      "metadata": {
        "id": "L_02wDgeih27"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install evaluate\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq, Trainer, TrainingArguments\n",
        "import evaluate\n",
        "import numpy as np\n",
        "\n",
        "checkpoint = \"facebook/bart-large-cnn\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(checkpoint)\n",
        "\n",
        "# Tokenization\n",
        "def preprocess(batch):\n",
        "    inputs = tokenizer(batch[\"article\"],\n",
        "                       max_length=512,\n",
        "                       truncation=True,\n",
        "                       padding=True)\n",
        "\n",
        "    with tokenizer.as_target_tokenizer():\n",
        "        labels = tokenizer(batch[\"highlights\"],\n",
        "                           max_length=128,\n",
        "                           truncation=True,\n",
        "                           padding=True)\n",
        "    inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "    return inputs\n",
        "\n",
        "# Dataset.map - applying the tokenizer function to each element of the dataset\n",
        "tokenized_train = small_train.map(\n",
        "    preprocess,\n",
        "    batched=True,\n",
        "    remove_columns=[\"article\", \"highlights\", \"id\"]  # Remove original columns\n",
        ")\n",
        "tokenized_val = small_val.map(\n",
        "    preprocess,\n",
        "    batched=True,\n",
        "    remove_columns=[\"article\", \"highlights\", \"id\"]  # Remove original columns\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "id": "EZ7t8Yf11llJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install rouge_score\n",
        "#!pip install evaluate\n",
        "import os\n",
        "import gc\n",
        "import glob\n",
        "import numpy as np\n",
        "import torch\n",
        "import evaluate\n",
        "from transformers import (\n",
        "    Seq2SeqTrainer,\n",
        "    Seq2SeqTrainingArguments,\n",
        "    DataCollatorForSeq2Seq,\n",
        ")\n",
        "\n",
        "# Cleaning GPU (working on Colab)\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "\n",
        "# Might be useful for VRAM management\n",
        "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Complicated metrics function with exceptions to prevent overflow which happened without them\n",
        "rouge = evaluate.load(\"rouge\")\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    predictions, labels = eval_pred\n",
        "\n",
        "    print(f\"Predictions shape: {predictions.shape}\")\n",
        "    print(f\"Predictions dtype: {predictions.dtype}\")\n",
        "    print(f\"Predictions min/max: {predictions.min()}/{predictions.max()}\")\n",
        "\n",
        "    # Handle predictions - they might be logits, so take argmax\n",
        "    if predictions.ndim == 3:  # If predictions are logits (batch_size, seq_len, vocab_size)\n",
        "        print(\"Taking argmax of 3D predictions (logits)\")\n",
        "        predictions = np.argmax(predictions, axis=-1)\n",
        "\n",
        "    print(f\"After processing - Predictions min/max: {predictions.min()}/{predictions.max()}\")\n",
        "\n",
        "    # Clip predictions to valid token ID range\n",
        "    max_token_id = tokenizer.vocab_size - 1 if hasattr(tokenizer, 'vocab_size') else 50000\n",
        "    print(f\"Tokenizer vocab size: {getattr(tokenizer, 'vocab_size', 'Unknown')}\")\n",
        "    print(f\"Clipping to max_token_id: {max_token_id}\")\n",
        "\n",
        "    predictions = np.clip(predictions, 0, max_token_id)\n",
        "\n",
        "    # Replace -100s in predictions with pad token id\n",
        "    predictions = np.where(predictions != -100, predictions, tokenizer.pad_token_id)\n",
        "\n",
        "    # Ensure predictions are int32 and within valid range\n",
        "    predictions = predictions.astype(np.int32)\n",
        "\n",
        "    try:\n",
        "        # Decode predictions\n",
        "        decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
        "        print(f\"Successfully decoded {len(decoded_preds)} predictions\")\n",
        "    except (OverflowError, ValueError) as e:\n",
        "        print(f\"Error decoding predictions: {e}\")\n",
        "        # Fallback: create empty predictions\n",
        "        decoded_preds = [\"\"] * len(predictions)\n",
        "\n",
        "    # Decode labels (replace -100 with pad token)\n",
        "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
        "    labels = np.clip(labels, 0, max_token_id)  # Clip labels too for safety\n",
        "    labels = labels.astype(np.int32)\n",
        "\n",
        "    try:\n",
        "        decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "        print(f\"Successfully decoded {len(decoded_labels)} labels\")\n",
        "    except (OverflowError, ValueError) as e:\n",
        "        print(f\"Error decoding labels: {e}\")\n",
        "        # Fallback: create empty labels\n",
        "        decoded_labels = [\"\"] * len(labels)\n",
        "\n",
        "    # Compute ROUGE only if we have valid decoded text\n",
        "    if any(decoded_preds) and any(decoded_labels):\n",
        "        try:\n",
        "            result = rouge.compute(\n",
        "                predictions=decoded_preds,\n",
        "                references=decoded_labels,\n",
        "                use_stemmer=True\n",
        "            )\n",
        "\n",
        "            print(f\"ROUGE scores computed successfully\")\n",
        "            # Return selected ROUGE scores\n",
        "            return {\n",
        "                \"rouge1\": result[\"rouge1\"],\n",
        "                \"rouge2\": result[\"rouge2\"],\n",
        "                \"rougeL\": result[\"rougeL\"],\n",
        "            }\n",
        "        except Exception as e:\n",
        "            print(f\"Error computing ROUGE: {e}\")\n",
        "            return {\n",
        "                \"rouge1\": 0.0,\n",
        "                \"rouge2\": 0.0,\n",
        "                \"rougeL\": 0.0,\n",
        "            }\n",
        "    else:\n",
        "        print(\"No valid decoded text found, returning zero scores\")\n",
        "        # Return zero scores if decoding failed\n",
        "        return {\n",
        "            \"rouge1\": 0.0,\n",
        "            \"rouge2\": 0.0,\n",
        "            \"rougeL\": 0.0,\n",
        "        }\n",
        "\n",
        "\n",
        "# Data collator\n",
        "collator = DataCollatorForSeq2Seq(\n",
        "    tokenizer, model=model, padding=True\n",
        ")\n",
        "\n",
        "\n",
        "# Training arguments\n",
        "args = Seq2SeqTrainingArguments(\n",
        "    output_dir=\"/content/drive/MyDrive/model_checkpoints\",\n",
        "    save_strategy=\"steps\",\n",
        "    save_steps=500,\n",
        "    save_total_limit=3,\n",
        "    eval_strategy=\"steps\",\n",
        "    eval_steps=500,\n",
        "    per_device_train_batch_size=2,\n",
        "    gradient_accumulation_steps=4,\n",
        "    per_device_eval_batch_size=1,\n",
        "    eval_accumulation_steps=16,\n",
        "    predict_with_generate=True,\n",
        "    generation_num_beams=2,\n",
        "    generation_max_length=256,\n",
        "    num_train_epochs=2,\n",
        "    learning_rate=3e-5,\n",
        "    warmup_steps=500,\n",
        "    weight_decay=0.01,\n",
        "    fp16=True,\n",
        "    logging_strategy=\"steps\",\n",
        "    logging_steps=100,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"rouge1\",\n",
        "    greater_is_better=True,\n",
        "    report_to=[\"tensorboard\"],\n",
        "    logging_dir=\"logs\",\n",
        ")\n",
        "\n",
        "# Trainer\n",
        "trainer = Seq2SeqTrainer(\n",
        "    model=model,\n",
        "    args=args,\n",
        "    train_dataset=tokenized_train,\n",
        "    eval_dataset=tokenized_val,\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=collator,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "# Resuming from checkpoint (in my case, 3600)\n",
        "resume_from = \"/content/checkpoint-NUMBER\"\n",
        "\n",
        "# Check if it exists\n",
        "if os.path.exists(resume_from):\n",
        "    print(f\"Using checkpoint: {resume_from}\")\n",
        "    trainer.train(resume_from_checkpoint=resume_from)\n",
        "else:\n",
        "    print(\"Checkpoint not found, starting fresh\")\n",
        "    trainer.train()\n",
        "\n",
        "\n",
        "# Save the model\n",
        "trainer.save_model(\"final_model\")\n",
        "tokenizer.save_pretrained(\"final_model\")"
      ],
      "metadata": {
        "id": "LOZ9Qoe9LYII"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
        "\n",
        "# Load model and tokenizer from a specific checkpoint\n",
        "checkpoint_path = \"/content/your-checkpoint-path\"\n",
        "\n",
        "print(\"Loading model from checkpoint...\")\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(checkpoint_path)\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint_path)\n",
        "\n",
        "print(\"Model loaded successfully!\")\n"
      ],
      "metadata": {
        "id": "10fo9rtmfDh_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test function for news summarization\n",
        "def test_summarization(article_text, max_length=128):\n",
        "    inputs = tokenizer(article_text, return_tensors=\"pt\", truncation=True, max_length=512)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            inputs.input_ids,\n",
        "            max_length=max_length,\n",
        "            num_beams=4,\n",
        "            early_stopping=True,\n",
        "            do_sample=False,\n",
        "            length_penalty=1.0\n",
        "        )\n",
        "\n",
        "    summary = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    return summary\n",
        "\n",
        "# Test 1: Technology News\n",
        "test_article_1 = \"\"\"\n",
        "Apple announced today that it will be releasing a major software update for its iPhone lineup next month. The iOS 18.2 update includes several new artificial intelligence features, improved battery management, and enhanced security protocols. The company's CEO stated that this update represents the most significant advancement in iPhone software in the past three years. Early beta testers have reported improvements in app performance and faster charging speeds. The update will be available for iPhone 12 and newer models, with older devices receiving a limited version of the features. Apple's stock price rose 3% following the announcement, as investors showed confidence in the company's continued innovation in the mobile technology sector.\n",
        "\"\"\"\n",
        "\n",
        "print(\"=== TEST 1: Technology News ===\")\n",
        "print(\"Original article length:\", len(test_article_1.split()))\n",
        "print(\"\\nGenerated summary:\")\n",
        "summary_1 = test_summarization(test_article_1)\n",
        "print(summary_1)\n",
        "print(f\"\\nSummary length: {len(summary_1.split())} words\")\n",
        "\n",
        "# Test 2: Politics/Economy News\n",
        "test_article_2 = \"\"\"\n",
        "The Federal Reserve announced a 0.25% interest rate cut yesterday, marking the third reduction this year as officials attempt to stimulate economic growth amid concerns about global trade tensions. Fed Chair Jerome Powell explained that the decision was made to support continued expansion and maintain price stability. Economists had mixed reactions to the announcement, with some arguing that further cuts may be necessary while others warned about potential inflationary pressures. The stock market responded positively, with major indices gaining over 2% in after-hours trading. Small businesses are expected to benefit from lower borrowing costs, while savers may see reduced returns on deposits. The next Federal Open Market Committee meeting is scheduled for December, where additional rate changes will be considered based on economic data and employment figures.\n",
        "\"\"\"\n",
        "\n",
        "print(\"\\n\\n=== TEST 2: Politics/Economy News ===\")\n",
        "print(\"Original article length:\", len(test_article_2.split()))\n",
        "print(\"\\nGenerated summary:\")\n",
        "summary_2 = test_summarization(test_article_2)\n",
        "print(summary_2)\n",
        "print(f\"\\nSummary length: {len(summary_2.split())} words\")\n",
        "\n",
        "def clean_summary(text):\n",
        "    # Remove spaces before punctuation\n",
        "    import re\n",
        "    text = re.sub(r'\\s+([.,!?;:])', r'\\1', text)\n",
        "    # Clean up multiple spaces\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "    return text.strip()\n",
        "\n",
        "# Apply to your summary\n",
        "clean_summary_2 = clean_summary(summary_2)\n",
        "print(\"Cleaned:\", clean_summary_2)\n"
      ],
      "metadata": {
        "id": "iKvLFtfBfOzk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}